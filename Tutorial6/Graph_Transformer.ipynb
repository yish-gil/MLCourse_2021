{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "import uproot#3 as uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For node pair (𝑖,𝑗) (from 𝑖 to 𝑗) with node $𝑥_𝑖$, $𝑥_𝑗$ $ \\in \\mathcal{R}_𝑛$, the score of their connection is defined as :\\\n",
    "### $\\bf 𝑞_𝑗=𝑊_𝑞⋅𝑥_𝑗$, $\\bf 𝑘_𝑖=𝑊_𝑘⋅𝑥_𝑖$, $\\bf 𝑣_𝑖=𝑊_𝑣⋅𝑥_𝑖$, $\\bf score=𝑞^{𝑇}_{𝑗}.𝑘_𝑖$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Multi-Head Attention\"\n",
    "    def __init__(self, h, dim_model):\n",
    "        \"h: number of heads; dim_model: hidden dimension\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_k = dim_model // h\n",
    "        self.h = h\n",
    "        # W_q, W_k, W_v, W_o\n",
    "        self.linears = clones(nn.Linear(dim_model, dim_model), 4)\n",
    "\n",
    "    def get(self, x, fields='qkv'):\n",
    "        \"Return a dict of queries / keys / values.\"\n",
    "        batch_size = x.shape[0]\n",
    "        ret = {}\n",
    "        if 'q' in fields:\n",
    "            ret['q'] = self.linears[0](x).view(batch_size, self.h, self.d_k)\n",
    "        if 'k' in fields:\n",
    "            ret['k'] = self.linears[1](x).view(batch_size, self.h, self.d_k)\n",
    "        if 'v' in fields:\n",
    "            ret['v'] = self.linears[2](x).view(batch_size, self.h, self.d_k)\n",
    "        return ret\n",
    "\n",
    "    def get_o(self, x):\n",
    "        \"get output of the multi-head attention\"\n",
    "        batch_size = x.shape[0]\n",
    "        return self.linears[3](x.view(batch_size, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_func(edges):\n",
    "    return {'score': ((edges.src['k'] * edges.dst['q'])\n",
    "                      .sum(-1, keepdim=True)),\n",
    "            'v': edges.src['v']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def reduce_func(nodes, d_k=64):\n",
    "    v = nodes.mailbox['v']\n",
    "    att = F.softmax(nodes.mailbox['score'] / th.sqrt(d_k), 1)\n",
    "    return {'dx': (att * v).sum(1)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functools.partial as partial\n",
    "def naive_propagate_attention(self, g, eids):\n",
    "    g.send_and_recv(eids, message_func, reduce_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def src_dot_dst(src_field, dst_field, out_field):\n",
    "    def func(edges):\n",
    "        return {out_field: (edges.src[src_field] * edges.dst[dst_field]).sum(-1, keepdim=True)}\n",
    "\n",
    "    return func\n",
    "\n",
    "def scaled_exp(field, scale_constant):\n",
    "    def func(edges):\n",
    "        # clamp for softmax numerical stability\n",
    "        return {field: th.exp((edges.data[field] / scale_constant).clamp(-5, 5))}\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def propagate_attention(self, g, eids):\n",
    "    # Compute attention score\n",
    "    g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n",
    "    g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n",
    "    # Update node state\n",
    "    g.send_and_recv(eids,\n",
    "                    [fn.src_mul_edge('v', 'score', 'v'), fn.copy_edge('score', 'score')],\n",
    "                    [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def pre_func(self, i, fields='qkv'):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            x = nodes.data['x']\n",
    "            norm_x = layer.sublayer[0].norm(x)\n",
    "            return layer.self_attn.get(norm_x, fields=fields)\n",
    "        return func\n",
    "\n",
    "    def post_func(self, i):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            x, wv, z = nodes.data['x'], nodes.data['wv'], nodes.data['z']\n",
    "            o = layer.self_attn.get_o(wv / z)\n",
    "            x = x + layer.sublayer[0].dropout(o)\n",
    "            x = layer.sublayer[1](x, layer.feed_forward)\n",
    "            return {'x': x if i < self.N - 1 else self.norm(x)}\n",
    "        return func\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.N = N\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def pre_func(self, i, fields='qkv', l=0):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            x = nodes.data['x']\n",
    "            if fields == 'kv':\n",
    "                norm_x = x # In enc-dec attention, x has already been normalized.\n",
    "            else:\n",
    "                norm_x = layer.sublayer[l].norm(x)\n",
    "            return layer.self_attn.get(norm_x, fields)\n",
    "        return func\n",
    "\n",
    "    def post_func(self, i, l=0):\n",
    "        layer = self.layers[i]\n",
    "        def func(nodes):\n",
    "            x, wv, z = nodes.data['x'], nodes.data['wv'], nodes.data['z']\n",
    "            o = layer.self_attn.get_o(wv / z)\n",
    "            x = x + layer.sublayer[l].dropout(o)\n",
    "            if l == 1:\n",
    "                x = layer.sublayer[2](x, layer.feed_forward)\n",
    "            return {'x': x if i < self.N - 1 else self.norm(x)}\n",
    "        return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, pos_enc, generator, h, d_k):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder, self.decoder = encoder, decoder\n",
    "        self.src_embed, self.tgt_embed = src_embed, tgt_embed\n",
    "        self.pos_enc = pos_enc\n",
    "        self.generator = generator\n",
    "        self.h, self.d_k = h, d_k\n",
    "\n",
    "    def propagate_attention(self, g, eids):\n",
    "        # Compute attention score\n",
    "        g.apply_edges(src_dot_dst('k', 'q', 'score'), eids)\n",
    "        g.apply_edges(scaled_exp('score', np.sqrt(self.d_k)))\n",
    "        # Send weighted values to target nodes\n",
    "        g.send_and_recv(eids,\n",
    "                        [fn.src_mul_edge('v', 'score', 'v'), fn.copy_edge('score', 'score')],\n",
    "                        [fn.sum('v', 'wv'), fn.sum('score', 'z')])\n",
    "\n",
    "    def update_graph(self, g, eids, pre_pairs, post_pairs):\n",
    "        \"Update the node states and edge states of the graph.\"\n",
    "\n",
    "        # Pre-compute queries and key-value pairs.\n",
    "        for pre_func, nids in pre_pairs:\n",
    "            g.apply_nodes(pre_func, nids)\n",
    "        self.propagate_attention(g, eids)\n",
    "        # Further calculation after attention mechanism\n",
    "        for post_func, nids in post_pairs:\n",
    "            g.apply_nodes(post_func, nids)\n",
    "\n",
    "    def forward(self, graph):\n",
    "        g = graph.g\n",
    "        nids, eids = graph.nids, graph.eids\n",
    "\n",
    "        # Word Embedding and Position Embedding\n",
    "        src_embed, src_pos = self.src_embed(graph.src[0]), self.pos_enc(graph.src[1])\n",
    "        tgt_embed, tgt_pos = self.tgt_embed(graph.tgt[0]), self.pos_enc(graph.tgt[1])\n",
    "        g.nodes[nids['enc']].data['x'] = self.pos_enc.dropout(src_embed + src_pos)\n",
    "        g.nodes[nids['dec']].data['x'] = self.pos_enc.dropout(tgt_embed + tgt_pos)\n",
    "\n",
    "        for i in range(self.encoder.N):\n",
    "            # Step 1: Encoder Self-attention\n",
    "            pre_func = self.encoder.pre_func(i, 'qkv')\n",
    "            post_func = self.encoder.post_func(i)\n",
    "            nodes, edges = nids['enc'], eids['ee']\n",
    "            self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n",
    "\n",
    "        for i in range(self.decoder.N):\n",
    "            # Step 2: Dncoder Self-attention\n",
    "            pre_func = self.decoder.pre_func(i, 'qkv')\n",
    "            post_func = self.decoder.post_func(i)\n",
    "            nodes, edges = nids['dec'], eids['dd']\n",
    "            self.update_graph(g, edges, [(pre_func, nodes)], [(post_func, nodes)])\n",
    "            # Step 3: Encoder-Decoder attention\n",
    "            pre_q = self.decoder.pre_func(i, 'q', 1)\n",
    "            pre_kv = self.decoder.pre_func(i, 'kv', 1)\n",
    "            post_func = self.decoder.post_func(i, 1)\n",
    "            nodes_e, nodes_d, edges = nids['enc'], nids['dec'], eids['ed']\n",
    "            self.update_graph(g, edges, [(pre_q, nodes_d), (pre_kv, nodes_e)], [(post_func, nodes_d)])\n",
    "\n",
    "        return self.generator(g.ndata['x'][nids['dec']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dgl.contrib.transformer import get_dataset, GraphPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
